{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smirzap/AI-dengan-DL/blob/main/food101_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSFER LEARNING\n",
        "\n",
        "*Lihat Buku BAB 11*\n",
        "\n",
        "**Transfer Learning** adalah metode Machine Learning yang memungkinkan model-model yang telah dikembangkan untuk suatu tugas tertentu digunakan kembali sebagai titik awal untuk (membuat) model pada tugas-tugas lain yang serupa.\n",
        "\n",
        "\n",
        "Tiga strategy Transfer Learning:\n",
        "1. **Strategi 1** yang melatih ulang semua lapisan atau parameter pada jaringan\n",
        "2. **Strategi 2** yang melatih sebagian lapisan pada jaringan, yaitu lapisan-lapisan pada bagian belakang dan lapisan FC/DNN.\n",
        "3. **Strategi 3** yang melatih hanya bagian classifier atau lapisan FC/DNN saja.\n",
        "\n",
        "Terdapat tiga eksperimen sesuai dengan jenis strategi penggunaan Transfer Learning. Sumber data berasal dari Food101:\n",
        "* Dipilih 10 kategori makanan (dari 101 kategori) dengan masing-masing kategori berisikan 75 gambar pada training set dan 250 gambar pada test set.\n",
        "* Sepuluh kategori makanan pada training set adalah chicken_curry, chicken_wings, fried_rice, grilled_salmon, hamburger, ice_cream, pizza, ramen, steak, dan sushi.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KirV6nbYYE9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Pastikan Pengunaan GPU"
      ],
      "metadata": {
        "id": "Q9zgPoFkYIyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6ZNVcfY_2H4G",
        "outputId": "2778cfd2-301f-4ac9-da0a-5bc50a73ba4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan Anda menggunakan GPU dengan perintah berikut\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brqi9iR61zMv",
        "outputId": "28064f2f-1704-4967-90dd-2f5f942f5ddf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 30 10:35:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Unduh Dataset Food101 dengan 10 Label"
      ],
      "metadata": {
        "id": "DxMS6FviYMBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m4oritD50WB0"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Ambil dataset Food101 yang telah ditetapkan 10 kategori makanan\n",
        "# Kemudian unzip data tersebut\n",
        "\n",
        "food_file = '/content/drive/MyDrive/10_food_classes_10_percent.zip'\n",
        "with ZipFile(food_file) as zip:\n",
        "  zip.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat nama-nama direktori yang berisikan dataset\n",
        "!ls 10_food_classes_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlJzbdtL5xMm",
        "outputId": "9925866a-ffd9-4959-ad68-5ee0be4393b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '10_food_classes_10_percent/'\n",
        "training_set_dir = dir_path + 'train/'\n",
        "test_set_dir = dir_path + 'test/'"
      ],
      "metadata": {
        "id": "RJ6G0Pz59ABi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Catatan Kode di bawah:**\n",
        "\n",
        "`os.walk()` adalah fungsi untuk mengitari secara rekursif sebuah pohon direktori dan mengembalikan tuple yang terdiri dari 3 komponen, yaitu `dirpath`, `dirnames`, `filenames`, untuk setiap direktori di pohon.\n",
        "*   `dirpath` adalah jalur ke direktori\n",
        "*   `dirnames` adalah daftar nama subdirektori di direktori\n",
        "* `filenames` adalah daftar nama file di direktori"
      ],
      "metadata": {
        "id": "7EiWE207Q5LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Mari kita lihat jumlah example pada training set dan test set\n",
        "for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"Terdapat {len(dirnames)} direktori and {len(filenames)} file gambar di '{dirpath}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEAeA4rD8Hsd",
        "outputId": "563c89ab-a5db-4fc3-ef85-72112b5dfcab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terdapat 2 direktori and 0 file gambar di '10_food_classes_10_percent/'.\n",
            "Terdapat 10 direktori and 0 file gambar di '10_food_classes_10_percent/test'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/sushi'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/ice_cream'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/chicken_wings'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/chicken_curry'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/ramen'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/hamburger'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/fried_rice'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/steak'.\n",
            "Terdapat 0 direktori and 250 file gambar di '10_food_classes_10_percent/test/pizza'.\n",
            "Terdapat 10 direktori and 0 file gambar di '10_food_classes_10_percent/train'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/sushi'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/ice_cream'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/chicken_wings'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/chicken_curry'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/ramen'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/hamburger'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/fried_rice'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/steak'.\n",
            "Terdapat 0 direktori and 75 file gambar di '10_food_classes_10_percent/train/pizza'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Buat Training dan Test Set"
      ],
      "metadata": {
        "id": "i4bIXl1TYRjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Catatan kode di bawah:**\n",
        "\n",
        "`image_dataset_from_directory()` adalah fungsi TensorFlow yang digunakan untuk membuat dataset gambar dari direktori."
      ],
      "metadata": {
        "id": "RHg0tJ6zRCdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fungsi untuk penyiapan dataset dalam format TensorFlow\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# ukuran gambar input yang akan diproses (tinggi, lebar)\n",
        "# jika ukuran berbeda maka akan dibuat menjadi ukuran ini\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# ukuran batch (jumlah gambar perbatch) yang akan dibuat\n",
        "# jika gunakan nilai default = 32\n",
        "# maka batch yang terdiri dari 32 gambar akan dikirimkan ke model pada setiap putaran\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Penyiapan training set\n",
        "# Lihat buku untuk penjelasan setiap parameter\n",
        "training_set = image_dataset_from_directory(directory=training_set_dir,\n",
        "                                            image_size=IMAGE_SIZE,\n",
        "                                            label_mode='categorical',\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            validation_split=0.3,\n",
        "                                            subset='training',\n",
        "                                            seed=42)\n",
        "\n",
        "# Penyiapan validation set (30% dari training set)\n",
        "validation_set = image_dataset_from_directory(directory=training_set_dir,\n",
        "                                            image_size=IMAGE_SIZE,\n",
        "                                            label_mode='categorical',\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            validation_split=0.3,\n",
        "                                            subset='validation',\n",
        "                                            seed=42)\n",
        "\n",
        "# Penyiapan test set\n",
        "test_set = image_dataset_from_directory(directory=test_set_dir,\n",
        "                                        image_size=IMAGE_SIZE,\n",
        "                                        label_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF83g_iZ7MPJ",
        "outputId": "27721dad-fbce-4418-9a2d-b02f694b8cbd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Using 525 files for training.\n",
            "Found 750 files belonging to 10 classes.\n",
            "Using 225 files for validation.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lihat 10 nama kategori makanan pada dataset\n",
        "training_set.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPbxSe_dANPo",
        "outputId": "2d58b595-7510-4148-da0e-3bcedc4ff3c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Catatan untuk output di bawah ini:**\n",
        "* `(None, 224, 224, 3)` mengacu pada bentuk tensor dari gambar kita, di mana `None` adalah ukuran batch, `224` adalah tinggi dan lebar gambar serta `3` adalah jumlah saluran warna (merah, hijau, biru)\n",
        "* Baik tensor gambar dan label memiliki tipe data `tf.float32`\n",
        "* Nilai `batch_size `adalah `None` karena hanya digunakan selama pelatihan model. Anda dapat menganggap `None` sebagai placeholder yang menunggu untuk diisi dengan parameter `batch_size` dari fungsi `image_dataset_from_directory()` saat model dilatih\n",
        "* Penggunaan nilai placeholder `None` untuk `batch_size` memiliki beberapa keuntungan:\n",
        "\n",
        "  1. Memungkinkan untuk menentukan nilai `batch_size` secara dinamis saat model dilatih\n",
        "  2. Memungkinkan untuk menggunakan nilai `batch_size` yang berbeda untuk berbagai tahap pelatihan model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rL70gjFWReD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (None, 224, 224, 3) refers to the tensor shape of our images where None is the batch size, 224 is the height (and width) and 3 is the color channels (red, green, blue).\n",
        "# (None, 10) refers to the tensor shape of the labels where None is the batch size and 10 is the number of possible labels (the 10 different food classes).\n",
        "# Both image tensors and labels are of the datatype tf.float32.\n",
        "# The batch_size is None due to it only being used during model training. You can think of None as a placeholder waiting to be filled with the batch_size parameter from image_dataset_from_directory().\n",
        "\n",
        "training_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z850ZaW-DOp",
        "outputId": "7237dbc2-510e-4b8b-f4ed-930fd2fb58ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Catatan: eksperimen dimulai dari Strategi 3 diikuti oleh Strategi 2 dan Strategi 1.**"
      ],
      "metadata": {
        "id": "ofS4Tja7UIg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Strategi 3: Hanya Mengubah Output Layer"
      ],
      "metadata": {
        "id": "FSu8vMVGYAJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(32)\n",
        "\n",
        "#-------------\n",
        "# Mari kita buat model dengan metode Transfer Learning\n",
        "#------------\n",
        "\n",
        "#1 Pilih dan unduh pretrained model\n",
        "pretrained_model = tf.keras.applications.EfficientNetV2B0(weights='imagenet',\n",
        "                                                        include_top=False,\n",
        "                                                        pooling='avg')\n",
        "\n",
        "#2 Buat agar paramater model tidak bisa diubah (frozen)\n",
        "pretrained_model.trainable=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9XnxYnj_Yoa",
        "outputId": "e6be5c3b-d0d7-4af6-9fd2-dec9c6034dd6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "24274472/24274472 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Buat Input baru karena kita menggunakan training set sendiri yang berbeda dengan ImageNet\n",
        "inputs = Input(shape=(224,224,3), name='input_layer')\n",
        "\n",
        "# Berikan Input layer baru sebagai input pada pretrained-model\n",
        "revised_model_3 = pretrained_model(inputs)\n",
        "\n",
        "# Definisikan output layer baru (karena kita menggunakan strategi 3)\n",
        "outputs = Dense(10, activation='softmax', name='output_layer')(revised_model_3)\n",
        "\n",
        "# Buat Model dengan menghubungkan antara inputs dan outputs\n",
        "model_3 = Model(inputs, outputs, name='model_3')"
      ],
      "metadata": {
        "id": "oqZ753Q_BD3z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mari kita compile model_3 agar siap dilakukan\n",
        "# pelatihan untuk parameter jaringan (bagian output saja pada kasus ini)\n",
        "model_3.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics='accuracy')"
      ],
      "metadata": {
        "id": "UzSIeoVQEL46"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lihat struktur dan parameter model\n",
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5oonkNnEjhS",
        "outputId": "e565df5c-c6cc-43db-d066-044f6dbae778"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,932,122\n",
            "Trainable params: 12,810\n",
            "Non-trainable params: 5,919,312\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Catatan kode di bawah ini:**\n",
        "\n",
        "Perintah `%load_ext tensorboard` adalah perintah Jupyter yang memuat ekstensi TensorBoard ke notebook saat ini. Ekstensi ini memungkinkan Anda untuk memvisualisasi pelatihan dan evaluasi model pembelajaran mesin Anda menggunakan TensorBoard."
      ],
      "metadata": {
        "id": "gGMDarL4WB1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "# hapus log tensorboard dari hasil eksekusi sebelumnya jika ada\n",
        "!rm -rf /logs/fit/\n",
        "\n",
        "# load tensorboard dengan magic command Jupyter\n",
        "%load_ext tensorboard\n",
        "\n",
        "# tetapkan direktori penyimpanan data hasil tangkapan\n",
        "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# buat callback tensorboard\n",
        "tb_callbacks = TensorBoard(log_dir=log_folder)"
      ],
      "metadata": {
        "id": "2gSU1WuIgX4n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# buat EarlyStopping callback\n",
        "es_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# buat ModelCheckpoint callback\n",
        "mpt_callback= ModelCheckpoint(filepath='best_model_3.hdf5', save_best_only=True, verbose=1)\n"
      ],
      "metadata": {
        "id": "5_xu5N7il3PI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sekarang kita latih model_3 untuk mendapatkan parameter pada output layer\n",
        "# Wow, kita mendapat akurasi 83%! dengan menggunakan strategi 3\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import time\n",
        "start = time.perf_counter()\n",
        "\n",
        "model_3.fit(training_set,\n",
        "            epochs=100,\n",
        "            validation_data=validation_set,\n",
        "            callbacks = [tb_callbacks, es_callback, mpt_callback])\n",
        "\n",
        "elapsed = time.perf_counter() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z1lox_hEw6S",
        "outputId": "4d072637-5ed8-40be-b9bc-91a9998d1e43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 1.9710 - accuracy: 0.3600\n",
            "Epoch 1: val_loss improved from inf to 1.57035, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 21s 307ms/step - loss: 1.9710 - accuracy: 0.3600 - val_loss: 1.5704 - val_accuracy: 0.6533\n",
            "Epoch 2/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 1.3284 - accuracy: 0.6953\n",
            "Epoch 2: val_loss improved from 1.57035 to 1.18953, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 4s 185ms/step - loss: 1.3211 - accuracy: 0.6971 - val_loss: 1.1895 - val_accuracy: 0.7111\n",
            "Epoch 3/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.9834 - accuracy: 0.7871\n",
            "Epoch 3: val_loss improved from 1.18953 to 0.99460, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 128ms/step - loss: 0.9851 - accuracy: 0.7829 - val_loss: 0.9946 - val_accuracy: 0.7333\n",
            "Epoch 4/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.7907 - accuracy: 0.8320\n",
            "Epoch 4: val_loss improved from 0.99460 to 0.88634, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 138ms/step - loss: 0.7872 - accuracy: 0.8343 - val_loss: 0.8863 - val_accuracy: 0.7778\n",
            "Epoch 5/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.6803 - accuracy: 0.8594\n",
            "Epoch 5: val_loss improved from 0.88634 to 0.82024, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 2s 123ms/step - loss: 0.6783 - accuracy: 0.8590 - val_loss: 0.8202 - val_accuracy: 0.7689\n",
            "Epoch 6/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.6012 - accuracy: 0.8789\n",
            "Epoch 6: val_loss improved from 0.82024 to 0.76487, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 133ms/step - loss: 0.6002 - accuracy: 0.8781 - val_loss: 0.7649 - val_accuracy: 0.7956\n",
            "Epoch 7/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.5385 - accuracy: 0.9004\n",
            "Epoch 7: val_loss improved from 0.76487 to 0.73691, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 124ms/step - loss: 0.5349 - accuracy: 0.9029 - val_loss: 0.7369 - val_accuracy: 0.8044\n",
            "Epoch 8/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.4842 - accuracy: 0.9121\n",
            "Epoch 8: val_loss improved from 0.73691 to 0.70252, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 2s 121ms/step - loss: 0.4803 - accuracy: 0.9124 - val_loss: 0.7025 - val_accuracy: 0.8044\n",
            "Epoch 9/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.4293 - accuracy: 0.9180\n",
            "Epoch 9: val_loss improved from 0.70252 to 0.68425, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 2s 120ms/step - loss: 0.4319 - accuracy: 0.9162 - val_loss: 0.6843 - val_accuracy: 0.8133\n",
            "Epoch 10/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.4080 - accuracy: 0.9199\n",
            "Epoch 10: val_loss improved from 0.68425 to 0.66737, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 170ms/step - loss: 0.4056 - accuracy: 0.9181 - val_loss: 0.6674 - val_accuracy: 0.8044\n",
            "Epoch 11/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.3659 - accuracy: 0.9297\n",
            "Epoch 11: val_loss improved from 0.66737 to 0.65106, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 124ms/step - loss: 0.3625 - accuracy: 0.9314 - val_loss: 0.6511 - val_accuracy: 0.8178\n",
            "Epoch 12/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.3545 - accuracy: 0.9258\n",
            "Epoch 12: val_loss improved from 0.65106 to 0.64338, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 125ms/step - loss: 0.3532 - accuracy: 0.9276 - val_loss: 0.6434 - val_accuracy: 0.8178\n",
            "Epoch 13/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.3167 - accuracy: 0.9453\n",
            "Epoch 13: val_loss improved from 0.64338 to 0.63591, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 160ms/step - loss: 0.3218 - accuracy: 0.9429 - val_loss: 0.6359 - val_accuracy: 0.8356\n",
            "Epoch 14/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.9414\n",
            "Epoch 14: val_loss improved from 0.63591 to 0.63150, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 131ms/step - loss: 0.3086 - accuracy: 0.9429 - val_loss: 0.6315 - val_accuracy: 0.8222\n",
            "Epoch 15/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2857 - accuracy: 0.9531\n",
            "Epoch 15: val_loss improved from 0.63150 to 0.62198, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 124ms/step - loss: 0.2888 - accuracy: 0.9486 - val_loss: 0.6220 - val_accuracy: 0.8178\n",
            "Epoch 16/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2712 - accuracy: 0.9551\n",
            "Epoch 16: val_loss improved from 0.62198 to 0.62009, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 126ms/step - loss: 0.2729 - accuracy: 0.9543 - val_loss: 0.6201 - val_accuracy: 0.8356\n",
            "Epoch 17/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2633 - accuracy: 0.9570\n",
            "Epoch 17: val_loss improved from 0.62009 to 0.61864, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 127ms/step - loss: 0.2616 - accuracy: 0.9581 - val_loss: 0.6186 - val_accuracy: 0.8267\n",
            "Epoch 18/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2337 - accuracy: 0.9668\n",
            "Epoch 18: val_loss improved from 0.61864 to 0.60487, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 156ms/step - loss: 0.2346 - accuracy: 0.9657 - val_loss: 0.6049 - val_accuracy: 0.8356\n",
            "Epoch 19/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2234 - accuracy: 0.9629\n",
            "Epoch 19: val_loss improved from 0.60487 to 0.60250, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 132ms/step - loss: 0.2219 - accuracy: 0.9638 - val_loss: 0.6025 - val_accuracy: 0.8267\n",
            "Epoch 20/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2172 - accuracy: 0.9648\n",
            "Epoch 20: val_loss did not improve from 0.60250\n",
            "17/17 [==============================] - 2s 102ms/step - loss: 0.2164 - accuracy: 0.9657 - val_loss: 0.6029 - val_accuracy: 0.8267\n",
            "Epoch 21/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1958 - accuracy: 0.9746\n",
            "Epoch 21: val_loss improved from 0.60250 to 0.59820, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 125ms/step - loss: 0.1962 - accuracy: 0.9752 - val_loss: 0.5982 - val_accuracy: 0.8311\n",
            "Epoch 22/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1924 - accuracy: 0.9746\n",
            "Epoch 22: val_loss improved from 0.59820 to 0.59702, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 171ms/step - loss: 0.1915 - accuracy: 0.9752 - val_loss: 0.5970 - val_accuracy: 0.8311\n",
            "Epoch 23/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1843 - accuracy: 0.9844\n",
            "Epoch 23: val_loss improved from 0.59702 to 0.59193, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 2s 124ms/step - loss: 0.1832 - accuracy: 0.9848 - val_loss: 0.5919 - val_accuracy: 0.8311\n",
            "Epoch 24/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1627 - accuracy: 0.9824\n",
            "Epoch 24: val_loss improved from 0.59193 to 0.59024, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 125ms/step - loss: 0.1617 - accuracy: 0.9829 - val_loss: 0.5902 - val_accuracy: 0.8356\n",
            "Epoch 25/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1650 - accuracy: 0.9844\n",
            "Epoch 25: val_loss did not improve from 0.59024\n",
            "17/17 [==============================] - 2s 105ms/step - loss: 0.1635 - accuracy: 0.9848 - val_loss: 0.5943 - val_accuracy: 0.8311\n",
            "Epoch 26/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1527 - accuracy: 0.9863\n",
            "Epoch 26: val_loss did not improve from 0.59024\n",
            "17/17 [==============================] - 3s 142ms/step - loss: 0.1509 - accuracy: 0.9867 - val_loss: 0.5949 - val_accuracy: 0.8178\n",
            "Epoch 27/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1491 - accuracy: 0.9863\n",
            "Epoch 27: val_loss did not improve from 0.59024\n",
            "17/17 [==============================] - 3s 109ms/step - loss: 0.1491 - accuracy: 0.9867 - val_loss: 0.5905 - val_accuracy: 0.8311\n",
            "Epoch 28/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9902\n",
            "Epoch 28: val_loss improved from 0.59024 to 0.58689, saving model to best_model_3.hdf5\n",
            "17/17 [==============================] - 3s 123ms/step - loss: 0.1355 - accuracy: 0.9905 - val_loss: 0.5869 - val_accuracy: 0.8356\n",
            "Epoch 29/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1286 - accuracy: 0.9883\n",
            "Epoch 29: val_loss did not improve from 0.58689\n",
            "17/17 [==============================] - 2s 104ms/step - loss: 0.1310 - accuracy: 0.9867 - val_loss: 0.5923 - val_accuracy: 0.8311\n",
            "Epoch 30/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1324 - accuracy: 0.9824\n",
            "Epoch 30: val_loss did not improve from 0.58689\n",
            "17/17 [==============================] - 2s 104ms/step - loss: 0.1331 - accuracy: 0.9829 - val_loss: 0.5942 - val_accuracy: 0.8311\n",
            "Epoch 31/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1338 - accuracy: 0.9844\n",
            "Epoch 31: val_loss did not improve from 0.58689\n",
            "17/17 [==============================] - 3s 128ms/step - loss: 0.1317 - accuracy: 0.9848 - val_loss: 0.5889 - val_accuracy: 0.8400\n",
            "Epoch 32/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9863\n",
            "Epoch 32: val_loss did not improve from 0.58689\n",
            "17/17 [==============================] - 3s 120ms/step - loss: 0.1190 - accuracy: 0.9867 - val_loss: 0.5917 - val_accuracy: 0.8311\n",
            "Epoch 33/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9941\n",
            "Epoch 33: val_loss did not improve from 0.58689\n",
            "17/17 [==============================] - 3s 141ms/step - loss: 0.1116 - accuracy: 0.9943 - val_loss: 0.5992 - val_accuracy: 0.8311\n",
            "Epoch 33: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Elapsed %.3f seconds.' % elapsed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugAhahEBo4ML",
        "outputId": "e74e685b-475e-4b96-ce88-e0d6405de15c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed 115.620 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Harap sabar: tensorboard memerlukan waktu untuk tayang\n",
        "%tensorboard --logdir={log_folder}"
      ],
      "metadata": {
        "id": "6C3NDU97iT-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load best hyperparameters dari hasil pembelajaran model yang telah dilakukan\n",
        "best_model_3 = load_model('best_model_3.hdf5')\n",
        "\n",
        "# Lakukan evaluasi kinerja model dengan test set\n",
        "best_model_3.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYnRH_r61_dw",
        "outputId": "e4bc7ae9-654a-4412-8ce7-4ec9fb34b46b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 8s 74ms/step - loss: 0.4098 - accuracy: 0.8704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4097979962825775, 0.8704000115394592]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. Strategi 2: Trainable 10 Layer"
      ],
      "metadata": {
        "id": "HvWM1CurX56E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pretrained_model agar parameter jaringan bisa dilatih\n",
        "pretrained_model.trainable=True"
      ],
      "metadata": {
        "id": "rc3-R1C4GyFx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pretrained_model.trainable_variables))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ml72mBcNsb_",
        "outputId": "53f75052-dc85-4f48-ffaa-5e92e9b80757"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perhatikan bahwa semua layer pada pretrained model telah di frozen!\n",
        "for urutan, lapisan in enumerate(pretrained_model.layers):\n",
        "  print(urutan, lapisan.name, lapisan.trainable)"
      ],
      "metadata": {
        "id": "N_ZIUm8vHx0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b559a087-6880-4a16-d340-bfd08da55e45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1 True\n",
            "1 rescaling True\n",
            "2 normalization True\n",
            "3 stem_conv True\n",
            "4 stem_bn True\n",
            "5 stem_activation True\n",
            "6 block1a_project_conv True\n",
            "7 block1a_project_bn True\n",
            "8 block1a_project_activation True\n",
            "9 block2a_expand_conv True\n",
            "10 block2a_expand_bn True\n",
            "11 block2a_expand_activation True\n",
            "12 block2a_project_conv True\n",
            "13 block2a_project_bn True\n",
            "14 block2b_expand_conv True\n",
            "15 block2b_expand_bn True\n",
            "16 block2b_expand_activation True\n",
            "17 block2b_project_conv True\n",
            "18 block2b_project_bn True\n",
            "19 block2b_drop True\n",
            "20 block2b_add True\n",
            "21 block3a_expand_conv True\n",
            "22 block3a_expand_bn True\n",
            "23 block3a_expand_activation True\n",
            "24 block3a_project_conv True\n",
            "25 block3a_project_bn True\n",
            "26 block3b_expand_conv True\n",
            "27 block3b_expand_bn True\n",
            "28 block3b_expand_activation True\n",
            "29 block3b_project_conv True\n",
            "30 block3b_project_bn True\n",
            "31 block3b_drop True\n",
            "32 block3b_add True\n",
            "33 block4a_expand_conv True\n",
            "34 block4a_expand_bn True\n",
            "35 block4a_expand_activation True\n",
            "36 block4a_dwconv2 True\n",
            "37 block4a_bn True\n",
            "38 block4a_activation True\n",
            "39 block4a_se_squeeze True\n",
            "40 block4a_se_reshape True\n",
            "41 block4a_se_reduce True\n",
            "42 block4a_se_expand True\n",
            "43 block4a_se_excite True\n",
            "44 block4a_project_conv True\n",
            "45 block4a_project_bn True\n",
            "46 block4b_expand_conv True\n",
            "47 block4b_expand_bn True\n",
            "48 block4b_expand_activation True\n",
            "49 block4b_dwconv2 True\n",
            "50 block4b_bn True\n",
            "51 block4b_activation True\n",
            "52 block4b_se_squeeze True\n",
            "53 block4b_se_reshape True\n",
            "54 block4b_se_reduce True\n",
            "55 block4b_se_expand True\n",
            "56 block4b_se_excite True\n",
            "57 block4b_project_conv True\n",
            "58 block4b_project_bn True\n",
            "59 block4b_drop True\n",
            "60 block4b_add True\n",
            "61 block4c_expand_conv True\n",
            "62 block4c_expand_bn True\n",
            "63 block4c_expand_activation True\n",
            "64 block4c_dwconv2 True\n",
            "65 block4c_bn True\n",
            "66 block4c_activation True\n",
            "67 block4c_se_squeeze True\n",
            "68 block4c_se_reshape True\n",
            "69 block4c_se_reduce True\n",
            "70 block4c_se_expand True\n",
            "71 block4c_se_excite True\n",
            "72 block4c_project_conv True\n",
            "73 block4c_project_bn True\n",
            "74 block4c_drop True\n",
            "75 block4c_add True\n",
            "76 block5a_expand_conv True\n",
            "77 block5a_expand_bn True\n",
            "78 block5a_expand_activation True\n",
            "79 block5a_dwconv2 True\n",
            "80 block5a_bn True\n",
            "81 block5a_activation True\n",
            "82 block5a_se_squeeze True\n",
            "83 block5a_se_reshape True\n",
            "84 block5a_se_reduce True\n",
            "85 block5a_se_expand True\n",
            "86 block5a_se_excite True\n",
            "87 block5a_project_conv True\n",
            "88 block5a_project_bn True\n",
            "89 block5b_expand_conv True\n",
            "90 block5b_expand_bn True\n",
            "91 block5b_expand_activation True\n",
            "92 block5b_dwconv2 True\n",
            "93 block5b_bn True\n",
            "94 block5b_activation True\n",
            "95 block5b_se_squeeze True\n",
            "96 block5b_se_reshape True\n",
            "97 block5b_se_reduce True\n",
            "98 block5b_se_expand True\n",
            "99 block5b_se_excite True\n",
            "100 block5b_project_conv True\n",
            "101 block5b_project_bn True\n",
            "102 block5b_drop True\n",
            "103 block5b_add True\n",
            "104 block5c_expand_conv True\n",
            "105 block5c_expand_bn True\n",
            "106 block5c_expand_activation True\n",
            "107 block5c_dwconv2 True\n",
            "108 block5c_bn True\n",
            "109 block5c_activation True\n",
            "110 block5c_se_squeeze True\n",
            "111 block5c_se_reshape True\n",
            "112 block5c_se_reduce True\n",
            "113 block5c_se_expand True\n",
            "114 block5c_se_excite True\n",
            "115 block5c_project_conv True\n",
            "116 block5c_project_bn True\n",
            "117 block5c_drop True\n",
            "118 block5c_add True\n",
            "119 block5d_expand_conv True\n",
            "120 block5d_expand_bn True\n",
            "121 block5d_expand_activation True\n",
            "122 block5d_dwconv2 True\n",
            "123 block5d_bn True\n",
            "124 block5d_activation True\n",
            "125 block5d_se_squeeze True\n",
            "126 block5d_se_reshape True\n",
            "127 block5d_se_reduce True\n",
            "128 block5d_se_expand True\n",
            "129 block5d_se_excite True\n",
            "130 block5d_project_conv True\n",
            "131 block5d_project_bn True\n",
            "132 block5d_drop True\n",
            "133 block5d_add True\n",
            "134 block5e_expand_conv True\n",
            "135 block5e_expand_bn True\n",
            "136 block5e_expand_activation True\n",
            "137 block5e_dwconv2 True\n",
            "138 block5e_bn True\n",
            "139 block5e_activation True\n",
            "140 block5e_se_squeeze True\n",
            "141 block5e_se_reshape True\n",
            "142 block5e_se_reduce True\n",
            "143 block5e_se_expand True\n",
            "144 block5e_se_excite True\n",
            "145 block5e_project_conv True\n",
            "146 block5e_project_bn True\n",
            "147 block5e_drop True\n",
            "148 block5e_add True\n",
            "149 block6a_expand_conv True\n",
            "150 block6a_expand_bn True\n",
            "151 block6a_expand_activation True\n",
            "152 block6a_dwconv2 True\n",
            "153 block6a_bn True\n",
            "154 block6a_activation True\n",
            "155 block6a_se_squeeze True\n",
            "156 block6a_se_reshape True\n",
            "157 block6a_se_reduce True\n",
            "158 block6a_se_expand True\n",
            "159 block6a_se_excite True\n",
            "160 block6a_project_conv True\n",
            "161 block6a_project_bn True\n",
            "162 block6b_expand_conv True\n",
            "163 block6b_expand_bn True\n",
            "164 block6b_expand_activation True\n",
            "165 block6b_dwconv2 True\n",
            "166 block6b_bn True\n",
            "167 block6b_activation True\n",
            "168 block6b_se_squeeze True\n",
            "169 block6b_se_reshape True\n",
            "170 block6b_se_reduce True\n",
            "171 block6b_se_expand True\n",
            "172 block6b_se_excite True\n",
            "173 block6b_project_conv True\n",
            "174 block6b_project_bn True\n",
            "175 block6b_drop True\n",
            "176 block6b_add True\n",
            "177 block6c_expand_conv True\n",
            "178 block6c_expand_bn True\n",
            "179 block6c_expand_activation True\n",
            "180 block6c_dwconv2 True\n",
            "181 block6c_bn True\n",
            "182 block6c_activation True\n",
            "183 block6c_se_squeeze True\n",
            "184 block6c_se_reshape True\n",
            "185 block6c_se_reduce True\n",
            "186 block6c_se_expand True\n",
            "187 block6c_se_excite True\n",
            "188 block6c_project_conv True\n",
            "189 block6c_project_bn True\n",
            "190 block6c_drop True\n",
            "191 block6c_add True\n",
            "192 block6d_expand_conv True\n",
            "193 block6d_expand_bn True\n",
            "194 block6d_expand_activation True\n",
            "195 block6d_dwconv2 True\n",
            "196 block6d_bn True\n",
            "197 block6d_activation True\n",
            "198 block6d_se_squeeze True\n",
            "199 block6d_se_reshape True\n",
            "200 block6d_se_reduce True\n",
            "201 block6d_se_expand True\n",
            "202 block6d_se_excite True\n",
            "203 block6d_project_conv True\n",
            "204 block6d_project_bn True\n",
            "205 block6d_drop True\n",
            "206 block6d_add True\n",
            "207 block6e_expand_conv True\n",
            "208 block6e_expand_bn True\n",
            "209 block6e_expand_activation True\n",
            "210 block6e_dwconv2 True\n",
            "211 block6e_bn True\n",
            "212 block6e_activation True\n",
            "213 block6e_se_squeeze True\n",
            "214 block6e_se_reshape True\n",
            "215 block6e_se_reduce True\n",
            "216 block6e_se_expand True\n",
            "217 block6e_se_excite True\n",
            "218 block6e_project_conv True\n",
            "219 block6e_project_bn True\n",
            "220 block6e_drop True\n",
            "221 block6e_add True\n",
            "222 block6f_expand_conv True\n",
            "223 block6f_expand_bn True\n",
            "224 block6f_expand_activation True\n",
            "225 block6f_dwconv2 True\n",
            "226 block6f_bn True\n",
            "227 block6f_activation True\n",
            "228 block6f_se_squeeze True\n",
            "229 block6f_se_reshape True\n",
            "230 block6f_se_reduce True\n",
            "231 block6f_se_expand True\n",
            "232 block6f_se_excite True\n",
            "233 block6f_project_conv True\n",
            "234 block6f_project_bn True\n",
            "235 block6f_drop True\n",
            "236 block6f_add True\n",
            "237 block6g_expand_conv True\n",
            "238 block6g_expand_bn True\n",
            "239 block6g_expand_activation True\n",
            "240 block6g_dwconv2 True\n",
            "241 block6g_bn True\n",
            "242 block6g_activation True\n",
            "243 block6g_se_squeeze True\n",
            "244 block6g_se_reshape True\n",
            "245 block6g_se_reduce True\n",
            "246 block6g_se_expand True\n",
            "247 block6g_se_excite True\n",
            "248 block6g_project_conv True\n",
            "249 block6g_project_bn True\n",
            "250 block6g_drop True\n",
            "251 block6g_add True\n",
            "252 block6h_expand_conv True\n",
            "253 block6h_expand_bn True\n",
            "254 block6h_expand_activation True\n",
            "255 block6h_dwconv2 True\n",
            "256 block6h_bn True\n",
            "257 block6h_activation True\n",
            "258 block6h_se_squeeze True\n",
            "259 block6h_se_reshape True\n",
            "260 block6h_se_reduce True\n",
            "261 block6h_se_expand True\n",
            "262 block6h_se_excite True\n",
            "263 block6h_project_conv True\n",
            "264 block6h_project_bn True\n",
            "265 block6h_drop True\n",
            "266 block6h_add True\n",
            "267 top_conv True\n",
            "268 top_bn True\n",
            "269 top_activation True\n",
            "270 avg_pool True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sekarang kita unfrozen 10 layer pada bagian belakang lapisan convolution\n",
        "for urutan, lapisan in enumerate(pretrained_model.layers[:-5]):\n",
        "  lapisan.trainable=False"
      ],
      "metadata": {
        "id": "GzBUbaqFH447"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pretrained_model.trainable_variables))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmTFpbHaNfO6",
        "outputId": "dafafc4b-021c-4cfd-e98d-155409dffa2c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Berikan input layer sebagai input pada pretrained-model\n",
        "revised_model_2 = pretrained_model(inputs)\n",
        "\n",
        "# Definisikan output layer (strategi 2)\n",
        "outputs = Dense(10, activation='softmax', name='output_layer')(revised_model_2)\n",
        "\n",
        "# defisinikan model dengan strategi 3 dengan me-link inputs dan outputs\n",
        "model_2 = Model(inputs, outputs, name='model_2')"
      ],
      "metadata": {
        "id": "JYlLycycI2iq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mari kita compile model_2 agar siap dilakukan pelatihan untuk parameter jaringan\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(), #learning_rate=0.0001\n",
        "                metrics='accuracy')"
      ],
      "metadata": {
        "id": "0PEbADYJJ_LL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rqZ74cmKHYO",
        "outputId": "7e7cc41b-b916-4c9f-a1b2-a22be7717fa1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,932,122\n",
            "Trainable params: 261,130\n",
            "Non-trainable params: 5,670,992\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tb_callbacks = TensorBoard(log_dir=log_folder)"
      ],
      "metadata": {
        "id": "jGaITLveqTwD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpt_callback= ModelCheckpoint(filepath='best_model_2.hdf5', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "T-5rjuMv2aaw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sekarang kita latih model_3 untuk mendapatkan parameter pada output layer\n",
        "# Wow, kita mendapat akurasi 83%! dengan menggunakan strategi 3\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "model_2.fit(training_set,\n",
        "            epochs=100,\n",
        "            validation_data=validation_set,\n",
        "            callbacks = [tb_callbacks, es_callback, mpt_callback])\n",
        "\n",
        "elapsed = time.perf_counter() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G16B9nVIKDoT",
        "outputId": "62592525-8b01-4b5d-ec04-0374b61ec06b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 1.6163 - accuracy: 0.5371\n",
            "Epoch 1: val_loss improved from inf to 0.89253, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 14s 255ms/step - loss: 1.5997 - accuracy: 0.5467 - val_loss: 0.8925 - val_accuracy: 0.7689\n",
            "Epoch 2/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.6698 - accuracy: 0.8301\n",
            "Epoch 2: val_loss improved from 0.89253 to 0.65807, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 128ms/step - loss: 0.6651 - accuracy: 0.8343 - val_loss: 0.6581 - val_accuracy: 0.8222\n",
            "Epoch 3/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.4291 - accuracy: 0.9082\n",
            "Epoch 3: val_loss improved from 0.65807 to 0.58617, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 153ms/step - loss: 0.4333 - accuracy: 0.9067 - val_loss: 0.5862 - val_accuracy: 0.8489\n",
            "Epoch 4/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.9355\n",
            "Epoch 4: val_loss improved from 0.58617 to 0.55601, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 127ms/step - loss: 0.3103 - accuracy: 0.9371 - val_loss: 0.5560 - val_accuracy: 0.8578\n",
            "Epoch 5/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.2261 - accuracy: 0.9746\n",
            "Epoch 5: val_loss improved from 0.55601 to 0.55362, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 126ms/step - loss: 0.2241 - accuracy: 0.9752 - val_loss: 0.5536 - val_accuracy: 0.8622\n",
            "Epoch 6/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1638 - accuracy: 0.9766\n",
            "Epoch 6: val_loss improved from 0.55362 to 0.52738, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 127ms/step - loss: 0.1676 - accuracy: 0.9752 - val_loss: 0.5274 - val_accuracy: 0.8400\n",
            "Epoch 7/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9824\n",
            "Epoch 7: val_loss improved from 0.52738 to 0.51210, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 122ms/step - loss: 0.1433 - accuracy: 0.9810 - val_loss: 0.5121 - val_accuracy: 0.8622\n",
            "Epoch 8/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.1122 - accuracy: 0.9902\n",
            "Epoch 8: val_loss improved from 0.51210 to 0.51076, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 2s 124ms/step - loss: 0.1110 - accuracy: 0.9905 - val_loss: 0.5108 - val_accuracy: 0.8711\n",
            "Epoch 9/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0884 - accuracy: 0.9922\n",
            "Epoch 9: val_loss improved from 0.51076 to 0.50817, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 161ms/step - loss: 0.0879 - accuracy: 0.9924 - val_loss: 0.5082 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0756 - accuracy: 0.9902\n",
            "Epoch 10: val_loss did not improve from 0.50817\n",
            "17/17 [==============================] - 3s 142ms/step - loss: 0.0756 - accuracy: 0.9905 - val_loss: 0.5093 - val_accuracy: 0.8622\n",
            "Epoch 11/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0645 - accuracy: 0.9941\n",
            "Epoch 11: val_loss improved from 0.50817 to 0.50011, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 125ms/step - loss: 0.0667 - accuracy: 0.9943 - val_loss: 0.5001 - val_accuracy: 0.8622\n",
            "Epoch 12/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0546 - accuracy: 0.9961\n",
            "Epoch 12: val_loss improved from 0.50011 to 0.49662, saving model to best_model_2.hdf5\n",
            "17/17 [==============================] - 3s 127ms/step - loss: 0.0555 - accuracy: 0.9962 - val_loss: 0.4966 - val_accuracy: 0.8711\n",
            "Epoch 13/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0460 - accuracy: 0.9980\n",
            "Epoch 13: val_loss did not improve from 0.49662\n",
            "17/17 [==============================] - 2s 107ms/step - loss: 0.0460 - accuracy: 0.9981 - val_loss: 0.5055 - val_accuracy: 0.8622\n",
            "Epoch 14/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
            "Epoch 14: val_loss did not improve from 0.49662\n",
            "17/17 [==============================] - 3s 139ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.5248 - val_accuracy: 0.8489\n",
            "Epoch 15/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0419 - accuracy: 0.9980\n",
            "Epoch 15: val_loss did not improve from 0.49662\n",
            "17/17 [==============================] - 3s 124ms/step - loss: 0.0430 - accuracy: 0.9981 - val_loss: 0.5262 - val_accuracy: 0.8489\n",
            "Epoch 16/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0371 - accuracy: 0.9980\n",
            "Epoch 16: val_loss did not improve from 0.49662\n",
            "17/17 [==============================] - 2s 102ms/step - loss: 0.0369 - accuracy: 0.9981 - val_loss: 0.5061 - val_accuracy: 0.8578\n",
            "Epoch 17/100\n",
            "16/17 [===========================>..] - ETA: 0s - loss: 0.0317 - accuracy: 0.9980\n",
            "Epoch 17: val_loss did not improve from 0.49662\n",
            "17/17 [==============================] - 2s 103ms/step - loss: 0.0323 - accuracy: 0.9981 - val_loss: 0.5053 - val_accuracy: 0.8711\n",
            "Epoch 17: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Elapsed %.3f seconds.' % elapsed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAXO9M8pnbY",
        "outputId": "44a06ec0-fe6d-4c88-be01-c759e45f317a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed 63.642 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gunakan port berbeda (8008) untuk tayangkan tensorboard Strategi 2\n",
        "%tensorboard --logdir={log_folder} --port=8008"
      ],
      "metadata": {
        "id": "u55CH-P0NBVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best hyperparameters from previous fit process\n",
        "best_model_2 = load_model('best_model_2.hdf5')\n",
        "\n",
        "# Let's evaluate the model\n",
        "best_model_2.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trXBOoO72OYB",
        "outputId": "21d393ce-16b3-46e4-f6b4-c4d99411409c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 9s 85ms/step - loss: 0.3338 - accuracy: 0.8880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.333753377199173, 0.8880000114440918]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = test_set.class_names\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn-JLzDWxUNN",
        "outputId": "dfc18c2b-f156-4a8d-f8f9-bdd278e223f9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. Strategi 1: Seluruh Paramater Dilatih"
      ],
      "metadata": {
        "id": "F7jgAxCBwSnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pretrained_model agar parameter jaringan bisa dilatih\n",
        "pretrained_model.trainable=True"
      ],
      "metadata": {
        "id": "rR57Lo0crawn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perhatikan bahwa semua layer pada pretrained model telah di frozen!\n",
        "for urutan, lapisan in enumerate(pretrained_model.layers):\n",
        "  print(urutan, lapisan.name, lapisan.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gzW6VMNSTib",
        "outputId": "e129e493-3337-4572-f44a-79356c9aaf89"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1 True\n",
            "1 rescaling True\n",
            "2 normalization True\n",
            "3 stem_conv True\n",
            "4 stem_bn True\n",
            "5 stem_activation True\n",
            "6 block1a_project_conv True\n",
            "7 block1a_project_bn True\n",
            "8 block1a_project_activation True\n",
            "9 block2a_expand_conv True\n",
            "10 block2a_expand_bn True\n",
            "11 block2a_expand_activation True\n",
            "12 block2a_project_conv True\n",
            "13 block2a_project_bn True\n",
            "14 block2b_expand_conv True\n",
            "15 block2b_expand_bn True\n",
            "16 block2b_expand_activation True\n",
            "17 block2b_project_conv True\n",
            "18 block2b_project_bn True\n",
            "19 block2b_drop True\n",
            "20 block2b_add True\n",
            "21 block3a_expand_conv True\n",
            "22 block3a_expand_bn True\n",
            "23 block3a_expand_activation True\n",
            "24 block3a_project_conv True\n",
            "25 block3a_project_bn True\n",
            "26 block3b_expand_conv True\n",
            "27 block3b_expand_bn True\n",
            "28 block3b_expand_activation True\n",
            "29 block3b_project_conv True\n",
            "30 block3b_project_bn True\n",
            "31 block3b_drop True\n",
            "32 block3b_add True\n",
            "33 block4a_expand_conv True\n",
            "34 block4a_expand_bn True\n",
            "35 block4a_expand_activation True\n",
            "36 block4a_dwconv2 True\n",
            "37 block4a_bn True\n",
            "38 block4a_activation True\n",
            "39 block4a_se_squeeze True\n",
            "40 block4a_se_reshape True\n",
            "41 block4a_se_reduce True\n",
            "42 block4a_se_expand True\n",
            "43 block4a_se_excite True\n",
            "44 block4a_project_conv True\n",
            "45 block4a_project_bn True\n",
            "46 block4b_expand_conv True\n",
            "47 block4b_expand_bn True\n",
            "48 block4b_expand_activation True\n",
            "49 block4b_dwconv2 True\n",
            "50 block4b_bn True\n",
            "51 block4b_activation True\n",
            "52 block4b_se_squeeze True\n",
            "53 block4b_se_reshape True\n",
            "54 block4b_se_reduce True\n",
            "55 block4b_se_expand True\n",
            "56 block4b_se_excite True\n",
            "57 block4b_project_conv True\n",
            "58 block4b_project_bn True\n",
            "59 block4b_drop True\n",
            "60 block4b_add True\n",
            "61 block4c_expand_conv True\n",
            "62 block4c_expand_bn True\n",
            "63 block4c_expand_activation True\n",
            "64 block4c_dwconv2 True\n",
            "65 block4c_bn True\n",
            "66 block4c_activation True\n",
            "67 block4c_se_squeeze True\n",
            "68 block4c_se_reshape True\n",
            "69 block4c_se_reduce True\n",
            "70 block4c_se_expand True\n",
            "71 block4c_se_excite True\n",
            "72 block4c_project_conv True\n",
            "73 block4c_project_bn True\n",
            "74 block4c_drop True\n",
            "75 block4c_add True\n",
            "76 block5a_expand_conv True\n",
            "77 block5a_expand_bn True\n",
            "78 block5a_expand_activation True\n",
            "79 block5a_dwconv2 True\n",
            "80 block5a_bn True\n",
            "81 block5a_activation True\n",
            "82 block5a_se_squeeze True\n",
            "83 block5a_se_reshape True\n",
            "84 block5a_se_reduce True\n",
            "85 block5a_se_expand True\n",
            "86 block5a_se_excite True\n",
            "87 block5a_project_conv True\n",
            "88 block5a_project_bn True\n",
            "89 block5b_expand_conv True\n",
            "90 block5b_expand_bn True\n",
            "91 block5b_expand_activation True\n",
            "92 block5b_dwconv2 True\n",
            "93 block5b_bn True\n",
            "94 block5b_activation True\n",
            "95 block5b_se_squeeze True\n",
            "96 block5b_se_reshape True\n",
            "97 block5b_se_reduce True\n",
            "98 block5b_se_expand True\n",
            "99 block5b_se_excite True\n",
            "100 block5b_project_conv True\n",
            "101 block5b_project_bn True\n",
            "102 block5b_drop True\n",
            "103 block5b_add True\n",
            "104 block5c_expand_conv True\n",
            "105 block5c_expand_bn True\n",
            "106 block5c_expand_activation True\n",
            "107 block5c_dwconv2 True\n",
            "108 block5c_bn True\n",
            "109 block5c_activation True\n",
            "110 block5c_se_squeeze True\n",
            "111 block5c_se_reshape True\n",
            "112 block5c_se_reduce True\n",
            "113 block5c_se_expand True\n",
            "114 block5c_se_excite True\n",
            "115 block5c_project_conv True\n",
            "116 block5c_project_bn True\n",
            "117 block5c_drop True\n",
            "118 block5c_add True\n",
            "119 block5d_expand_conv True\n",
            "120 block5d_expand_bn True\n",
            "121 block5d_expand_activation True\n",
            "122 block5d_dwconv2 True\n",
            "123 block5d_bn True\n",
            "124 block5d_activation True\n",
            "125 block5d_se_squeeze True\n",
            "126 block5d_se_reshape True\n",
            "127 block5d_se_reduce True\n",
            "128 block5d_se_expand True\n",
            "129 block5d_se_excite True\n",
            "130 block5d_project_conv True\n",
            "131 block5d_project_bn True\n",
            "132 block5d_drop True\n",
            "133 block5d_add True\n",
            "134 block5e_expand_conv True\n",
            "135 block5e_expand_bn True\n",
            "136 block5e_expand_activation True\n",
            "137 block5e_dwconv2 True\n",
            "138 block5e_bn True\n",
            "139 block5e_activation True\n",
            "140 block5e_se_squeeze True\n",
            "141 block5e_se_reshape True\n",
            "142 block5e_se_reduce True\n",
            "143 block5e_se_expand True\n",
            "144 block5e_se_excite True\n",
            "145 block5e_project_conv True\n",
            "146 block5e_project_bn True\n",
            "147 block5e_drop True\n",
            "148 block5e_add True\n",
            "149 block6a_expand_conv True\n",
            "150 block6a_expand_bn True\n",
            "151 block6a_expand_activation True\n",
            "152 block6a_dwconv2 True\n",
            "153 block6a_bn True\n",
            "154 block6a_activation True\n",
            "155 block6a_se_squeeze True\n",
            "156 block6a_se_reshape True\n",
            "157 block6a_se_reduce True\n",
            "158 block6a_se_expand True\n",
            "159 block6a_se_excite True\n",
            "160 block6a_project_conv True\n",
            "161 block6a_project_bn True\n",
            "162 block6b_expand_conv True\n",
            "163 block6b_expand_bn True\n",
            "164 block6b_expand_activation True\n",
            "165 block6b_dwconv2 True\n",
            "166 block6b_bn True\n",
            "167 block6b_activation True\n",
            "168 block6b_se_squeeze True\n",
            "169 block6b_se_reshape True\n",
            "170 block6b_se_reduce True\n",
            "171 block6b_se_expand True\n",
            "172 block6b_se_excite True\n",
            "173 block6b_project_conv True\n",
            "174 block6b_project_bn True\n",
            "175 block6b_drop True\n",
            "176 block6b_add True\n",
            "177 block6c_expand_conv True\n",
            "178 block6c_expand_bn True\n",
            "179 block6c_expand_activation True\n",
            "180 block6c_dwconv2 True\n",
            "181 block6c_bn True\n",
            "182 block6c_activation True\n",
            "183 block6c_se_squeeze True\n",
            "184 block6c_se_reshape True\n",
            "185 block6c_se_reduce True\n",
            "186 block6c_se_expand True\n",
            "187 block6c_se_excite True\n",
            "188 block6c_project_conv True\n",
            "189 block6c_project_bn True\n",
            "190 block6c_drop True\n",
            "191 block6c_add True\n",
            "192 block6d_expand_conv True\n",
            "193 block6d_expand_bn True\n",
            "194 block6d_expand_activation True\n",
            "195 block6d_dwconv2 True\n",
            "196 block6d_bn True\n",
            "197 block6d_activation True\n",
            "198 block6d_se_squeeze True\n",
            "199 block6d_se_reshape True\n",
            "200 block6d_se_reduce True\n",
            "201 block6d_se_expand True\n",
            "202 block6d_se_excite True\n",
            "203 block6d_project_conv True\n",
            "204 block6d_project_bn True\n",
            "205 block6d_drop True\n",
            "206 block6d_add True\n",
            "207 block6e_expand_conv True\n",
            "208 block6e_expand_bn True\n",
            "209 block6e_expand_activation True\n",
            "210 block6e_dwconv2 True\n",
            "211 block6e_bn True\n",
            "212 block6e_activation True\n",
            "213 block6e_se_squeeze True\n",
            "214 block6e_se_reshape True\n",
            "215 block6e_se_reduce True\n",
            "216 block6e_se_expand True\n",
            "217 block6e_se_excite True\n",
            "218 block6e_project_conv True\n",
            "219 block6e_project_bn True\n",
            "220 block6e_drop True\n",
            "221 block6e_add True\n",
            "222 block6f_expand_conv True\n",
            "223 block6f_expand_bn True\n",
            "224 block6f_expand_activation True\n",
            "225 block6f_dwconv2 True\n",
            "226 block6f_bn True\n",
            "227 block6f_activation True\n",
            "228 block6f_se_squeeze True\n",
            "229 block6f_se_reshape True\n",
            "230 block6f_se_reduce True\n",
            "231 block6f_se_expand True\n",
            "232 block6f_se_excite True\n",
            "233 block6f_project_conv True\n",
            "234 block6f_project_bn True\n",
            "235 block6f_drop True\n",
            "236 block6f_add True\n",
            "237 block6g_expand_conv True\n",
            "238 block6g_expand_bn True\n",
            "239 block6g_expand_activation True\n",
            "240 block6g_dwconv2 True\n",
            "241 block6g_bn True\n",
            "242 block6g_activation True\n",
            "243 block6g_se_squeeze True\n",
            "244 block6g_se_reshape True\n",
            "245 block6g_se_reduce True\n",
            "246 block6g_se_expand True\n",
            "247 block6g_se_excite True\n",
            "248 block6g_project_conv True\n",
            "249 block6g_project_bn True\n",
            "250 block6g_drop True\n",
            "251 block6g_add True\n",
            "252 block6h_expand_conv True\n",
            "253 block6h_expand_bn True\n",
            "254 block6h_expand_activation True\n",
            "255 block6h_dwconv2 True\n",
            "256 block6h_bn True\n",
            "257 block6h_activation True\n",
            "258 block6h_se_squeeze True\n",
            "259 block6h_se_reshape True\n",
            "260 block6h_se_reduce True\n",
            "261 block6h_se_expand True\n",
            "262 block6h_se_excite True\n",
            "263 block6h_project_conv True\n",
            "264 block6h_project_bn True\n",
            "265 block6h_drop True\n",
            "266 block6h_add True\n",
            "267 top_conv True\n",
            "268 top_bn True\n",
            "269 top_activation True\n",
            "270 avg_pool True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Berikan input layer sebagai input pada pretrained-model\n",
        "revised_model_1 = pretrained_model(inputs)\n",
        "\n",
        "# Definisikan output layer (strategi 1)\n",
        "outputs = Dense(10, activation='softmax', name='output_layer')(revised_model_1)\n",
        "\n",
        "# defisinikan model dengan strategi 3 dengan me-link inputs dan outputs\n",
        "model_1 = Model(inputs, outputs, name='model_1')"
      ],
      "metadata": {
        "id": "xMuCEi88wYJx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mari kita compile model_1 agar siap dilakukan pelatihan untuk parameter jaringan\n",
        "model_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(), #learning_rate=0.0001\n",
        "                metrics='accuracy')"
      ],
      "metadata": {
        "id": "gMH_R4LFwm8t"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tb_callbacks = TensorBoard(log_dir=log_folder)"
      ],
      "metadata": {
        "id": "08ckYQ-iwyon"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpt_callback= ModelCheckpoint(filepath='best_model_1.hdf5', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "zpGlaUqe4m-o"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sekarang kita latih model_3 untuk mendapatkan parameter pada output layer\n",
        "# Wow, kita mendapat akurasi 83%! dengan menggunakan strategi 3\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "model_1.fit(training_set,\n",
        "            epochs=100,\n",
        "            validation_data=validation_set,\n",
        "            callbacks = [tb_callbacks, mpt_callback, es_callback])\n",
        "\n",
        "elapsed = time.perf_counter() - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nintfcEwoyn",
        "outputId": "0685d13f-d061-46b7-dc23-88881db2332a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 1.5730 - accuracy: 0.5048\n",
            "Epoch 1: val_loss improved from inf to 0.89514, saving model to best_model_1.hdf5\n",
            "17/17 [==============================] - 62s 466ms/step - loss: 1.5730 - accuracy: 0.5048 - val_loss: 0.8951 - val_accuracy: 0.7067\n",
            "Epoch 2/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 0.9029\n",
            "Epoch 2: val_loss improved from 0.89514 to 0.83861, saving model to best_model_1.hdf5\n",
            "17/17 [==============================] - 5s 274ms/step - loss: 0.3729 - accuracy: 0.9029 - val_loss: 0.8386 - val_accuracy: 0.6933\n",
            "Epoch 3/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9695\n",
            "Epoch 3: val_loss did not improve from 0.83861\n",
            "17/17 [==============================] - 5s 262ms/step - loss: 0.1276 - accuracy: 0.9695 - val_loss: 0.9204 - val_accuracy: 0.7244\n",
            "Epoch 4/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9733\n",
            "Epoch 4: val_loss did not improve from 0.83861\n",
            "17/17 [==============================] - 5s 239ms/step - loss: 0.0923 - accuracy: 0.9733 - val_loss: 1.0414 - val_accuracy: 0.6978\n",
            "Epoch 5/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9790\n",
            "Epoch 5: val_loss did not improve from 0.83861\n",
            "17/17 [==============================] - 4s 226ms/step - loss: 0.0804 - accuracy: 0.9790 - val_loss: 1.0052 - val_accuracy: 0.6889\n",
            "Epoch 6/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9733\n",
            "Epoch 6: val_loss did not improve from 0.83861\n",
            "17/17 [==============================] - 5s 251ms/step - loss: 0.0877 - accuracy: 0.9733 - val_loss: 1.1829 - val_accuracy: 0.6933\n",
            "Epoch 7/100\n",
            "17/17 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9581\n",
            "Epoch 7: val_loss did not improve from 0.83861\n",
            "17/17 [==============================] - 5s 223ms/step - loss: 0.1243 - accuracy: 0.9581 - val_loss: 1.4213 - val_accuracy: 0.6311\n",
            "Epoch 7: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pretrained_model.trainable_variables))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ft54_QHSrAo",
        "outputId": "da5657fe-88b5-4199-84a5-88c43104b770"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IJ-BmxEPpVS",
        "outputId": "2ba91847-267c-4ba2-a0a8-0261a0b66c3d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,932,122\n",
            "Trainable params: 5,871,514\n",
            "Non-trainable params: 60,608\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir={log_folder} --port=8010"
      ],
      "metadata": {
        "id": "LEmcd7a042AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best hyperparameters from previous fit process\n",
        "best_model_1 = load_model('best_model_1.hdf5')\n",
        "\n",
        "# Let's evaluate the model\n",
        "best_model_1.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GhNm_Om48-g",
        "outputId": "f89e561d-bab1-4756-e372-fd8eeff5b535"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 9s 74ms/step - loss: 0.6562 - accuracy: 0.7896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6561955809593201, 0.7896000146865845]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. Klasifikasi Foto Makanan Aktual\n"
      ],
      "metadata": {
        "id": "Wd0dLIjHi8x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.io.decode_image(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img"
      ],
      "metadata": {
        "id": "6lksPrKbnPur"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = test_set.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "j6LD_mnuxcFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e304da-03ad-4d6f-b2a5-01cc0f40c4d2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Ambil dataset dan unzip data tersebut\n",
        "!wget https://raw.githubusercontent.com/smirzap/AI-dengan-DL/main/data/my_food_collection.zip\n",
        "\n",
        "with ZipFile('my_food_collection.zip') as zip:\n",
        "  zip.extractall()"
      ],
      "metadata": {
        "id": "Qtv6LkXSjBxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ce0860-e8e0-44c0-fcfb-aa3701270bad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-30 11:06:09--  https://raw.githubusercontent.com/smirzap/AI-dengan-DL/main/data/my_food_collection.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2207554 (2.1M) [application/zip]\n",
            "Saving to: ‘my_food_collection.zip’\n",
            "\n",
            "my_food_collection. 100%[===================>]   2.10M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-30 11:06:10 (137 MB/s) - ‘my_food_collection.zip’ saved [2207554/2207554]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat list nama file berikut nama direktorinya\n",
        "custom_food_images = [\"my_food_collection/\" + foto for foto in os.listdir(\"my_food_collection\")]\n",
        "custom_food_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SsfU2DIoX-A",
        "outputId": "394f576e-e51c-4e10-8953-c609fbc2cc4c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my_food_collection/sushi 3.jpeg',\n",
              " 'my_food_collection/sushi 1.JPG',\n",
              " 'my_food_collection/ramen 2.jpeg',\n",
              " 'my_food_collection/ramen 1.jpeg',\n",
              " 'my_food_collection/beef burger.jpg',\n",
              " 'my_food_collection/sushi 2.JPG']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best hyperparameters from previous fit process\n",
        "best_model_2 = load_model('best_model_2.hdf5')"
      ],
      "metadata": {
        "id": "CqNfdOS7GGXi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on custom food images\n",
        "for img in custom_food_images:\n",
        "\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(img)\n",
        "\n",
        "  # Decode it into a tensor\n",
        "  img = tf.io.decode_image(img)\n",
        "\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [224, 224])\n",
        "\n",
        "  pred_prob = best_model_2.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]\n",
        "  pred_catg = pred_prob.argmax(axis=-1)\n",
        "  pred_class = class_names[pred_catg[0]] # find the predicted class label\n",
        "\n",
        "  # Plot the image with appropriate annotations\n",
        "  plt.figure()\n",
        "  plt.imshow(img/255.) # imshow() requires float inputs to be normalized\n",
        "  plt.title(f\"prediksi: {pred_class} ({pred_prob.max()*100:.2f}%)\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "k08_eez5xgs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on custom food images\n",
        "for img in custom_food_images:\n",
        "  img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor\n",
        "  pred_prob = best_model_3.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]\n",
        "  pred_class = class_names[pred_prob.argmax()] # find the predicted class label\n",
        "  # Plot the image with appropriate annotations\n",
        "  plt.figure(dpi=100)\n",
        "  plt.imshow(img/255.) # imshow() requires float inputs to be normalized\n",
        "  plt.title(f\"prediksi: {pred_class} ({pred_prob.max()*100:.2f}%)\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "-_CqvuAIzgmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZH63U9IfeuX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}